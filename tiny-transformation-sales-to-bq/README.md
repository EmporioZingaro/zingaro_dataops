# Tiny Transformation Sales to BigQuery

This Cloud Function consumes Pub/Sub messages generated by the Tiny pipeline and transforms sales payloads into reporting tables in BigQuery.

It writes two tables:
- **pedidos**: order-level reporting facts
- **produtos** (itens_pedido): item-level reporting facts

## Entry point

Set Cloud Function entry point to:

```text
process_pubsub_message
```

The handler supports both:
- classic background signature (`event, context`)
- HTTP/Eventarc-wrapped Pub/Sub payloads (single request argument), for safer deployments

## Input message contract

The Pub/Sub message must contain:

- `uuid`
- `timestamp`
- `store_prefix`
- `pdv_pedido_data`
- `produto_data`
- `pedidos_pesquisa_data`

Expected source is the `tiny_pipeline` publisher.

## Multi-store routing

Preferred mode is dataset-per-store:

- Dataset: `<normalized_store_prefix>_<DATASET_BASE_ID>`
- Tables: `<PEDIDOS_TABLE_NAME>` and `<ITENS_PEDIDO_TABLE_NAME>`

Example:
- `store_prefix = Z316`
- `DATASET_BASE_ID = tiny`
- dataset result = `z316_tiny`

`store_prefix` normalization:
- lowercase
- non-alphanumeric characters replaced with `_`
- trimmed `_` on edges

## Required environment variables

### Preferred (multi-store)

- `PROJECT_ID`: GCP project id.
- `DATASET_BASE_ID`: Dataset suffix/base name used in `<store_prefix>_<DATASET_BASE_ID>`.
- `SOURCE_ID`: Source value stored in output rows.

### Optional (table names)

- `PEDIDOS_TABLE_NAME` (default: `pedidos`)
- `ITENS_PEDIDO_TABLE_NAME` (default: `produtos`)
- `LOG_LEVEL` (default: `INFO`)

### Legacy fallback mode

If `DATASET_BASE_ID` is not provided, the function uses fixed full table ids:

- `PEDIDOS_TABLE_ID`
- `ITENS_PEDIDO_TABLE_ID`

## Behavior improvements in this refactor

- Multi-store dataset routing using `store_prefix`.
- Decimal-safe calculations for financial fields.
- Safer handling for 100% discounts and zero quantities.
- Better table existence handling (`NotFound`/`Conflict`).
- Idempotent streaming inserts using deterministic `insertId` values.
- Verbose operational logs for start, routing, transformation, and persistence phases.

## Notes

- Output schema is preserved to stay compatible with existing reporting tables.
- BigQuery tables are partitioned by `pedido_dia` and clustered for reporting filters.
- If `DATASET_BASE_ID` is set, `PEDIDOS_TABLE_ID` and `ITENS_PEDIDO_TABLE_ID` are ignored.
